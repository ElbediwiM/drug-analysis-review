{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=\"center\">\n",
    "\n",
    "#  Drug Reviews Analysis Sentiment Analysis with Word2Vec \n",
    "\n",
    "<div align=\"center\">\n",
    "\n",
    "## üåü Deep Learning for Natural Language Processing üåü\n",
    "\n",
    "**Advanced Text Analytics | Drug Review Analysis | Transfer Learning**\n",
    "\n",
    "---\n",
    "\n",
    "### üéØ **Project Objectives**\n",
    "1. üìä **Load Data** - Process WebMD drug review dataset\n",
    "2. üß† **Build Models** - Create Word2Vec embeddings & LSTM networks  \n",
    "3. ‚òÅÔ∏è **Deploy** - Upload to cloud platform\n",
    "\n",
    "---\n",
    "\n",
    "*Transforming text into insights through the power of neural networks* ‚ú®\n",
    "\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üìä Project Summary: Sentiment Analysis with Word2Vec\n",
    "\n",
    "## üéØ Main Goal\n",
    "**Building a system to analyze text reviews and predict satisfaction levels using deep learning.**\n",
    "\n",
    "---\n",
    "\n",
    "## üîÑ Step-by-Step Process\n",
    "\n",
    "### 1Ô∏è‚É£ **Data Loading** üìÇ\n",
    "- üì• Loads a WebMD dataset with drug reviews\n",
    "- üéØ Uses **\"Reviews\"** column as input text and **\"Satisfaction\"** column as target\n",
    "- üßπ Cleans the data and splits it into training/testing sets\n",
    "\n",
    "### 2Ô∏è‚É£ **Word2Vec Training (Model 1)** ü§ñ\n",
    "- üèãÔ∏è Trains your own Word2Vec model on the review text\n",
    "- üî¢ Converts words into numerical vectors that capture word meanings\n",
    "- üìê Creates embeddings with **60 dimensions** per word\n",
    "\n",
    "### 3Ô∏è‚É£ **Transfer Learning (Model 2)** üöÄ\n",
    "- ‚¨áÔ∏è Loads pre-trained model: **\"glove-wiki-gigaword-50\"**\n",
    "- üåê Model trained on Wikipedia with superior word representations\n",
    "- üìö Like using a dictionary that already knows word relationships\n",
    "\n",
    "### 4Ô∏è‚É£ **Text Processing** ‚öôÔ∏è\n",
    "- üîó Converts review sentences into sequences of word vectors\n",
    "- üìè Pads sequences to uniform length (**200 words max**)\n",
    "- üéõÔ∏è Prepares data for the neural network\n",
    "\n",
    "### 5Ô∏è‚É£ **Neural Network Training** üß†\n",
    "- üèóÔ∏è Builds an **LSTM** (Long Short-Term Memory) model\n",
    "- üìù LSTM excels at understanding text sequences\n",
    "- üéì Trains model to predict satisfaction from review text\n",
    "- ‚èπÔ∏è Uses early stopping to prevent overfitting\n",
    "\n",
    "### 6Ô∏è‚É£ **Evaluation** üìà\n",
    "- üß™ Tests model on unseen data for accuracy assessment\n",
    "- ‚öñÔ∏è Compares performance: Custom Word2Vec vs. Pre-trained embeddings\n",
    "\n",
    "---\n",
    "\n",
    "## üí° **Key Insight**\n",
    "> **Transfer learning** (using pre-trained embeddings) typically outperforms training from scratch because pre-trained models have been exposed to vastly more text data! üåü\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Data Loading and Libs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "Data_Set_webmd_6174_Rows_Randomised.csv file not found!",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 20\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file_path \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m---> 20\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mData_Set_webmd_6174_Rows_Randomised.csv file not found!\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m# --- 2. Try loading as Excel, CSV, TSV, and with different encodings ---\u001b[39;00m\n\u001b[1;32m     23\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: Data_Set_webmd_6174_Rows_Randomised.csv file not found!"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "import numpy as np\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.preprocessing.text import text_to_word_sequence\n",
    "\n",
    "# --- 1. Locate and load the file ---\n",
    "possible_paths = [\n",
    "    '../Toms_Raw_Data/Data_Set_webmd_6174_Rows_Randomised.csv'\n",
    " ]\n",
    "file_path = None\n",
    "for path in possible_paths:\n",
    "    if os.path.exists(path):\n",
    "        file_path = path\n",
    "        print(f'Found file: {file_path}')\n",
    "        break\n",
    "if file_path is None:\n",
    "    raise FileNotFoundError('Data_Set_webmd_6174_Rows_Randomised.csv file not found!')\n",
    "\n",
    "# --- 2. Try loading as Excel, CSV, TSV, and with different encodings ---\n",
    "df = None\n",
    "if file_path.endswith('.xlsx'):\n",
    "    try:\n",
    "        df = pd.read_excel(file_path)\n",
    "        print('Loaded as Excel')\n",
    "    except Exception as e:\n",
    "        print('Could not load as Excel:', e)\n",
    "if df is None:\n",
    "    encodings = ['utf-8', 'latin-1', 'iso-8859-1']\n",
    "    for enc in encodings:\n",
    "        try:\n",
    "            df = pd.read_csv(file_path, encoding=enc)\n",
    "            print(f'Loaded as CSV with encoding {enc}')\n",
    "            break\n",
    "        except Exception:\n",
    "            try:\n",
    "                df = pd.read_csv(file_path, sep='\\t', encoding=enc)\n",
    "                print(f'Loaded as TSV with encoding {enc}')\n",
    "                break\n",
    "            except Exception:\n",
    "                continue\n",
    "if df is None:\n",
    "    raise Exception('Could not load the file with any tried encoding or format.')\n",
    "\n",
    "print('Columns:', df.columns.tolist())\n",
    "print(df.head())\n",
    "\n",
    "# --- 3. Use 'Reviews' as input and 'Satisfaction' as output ---\n",
    "text_col = 'Reviews' #\"Sides\" for my Tom model\n",
    "label_col = 'Satisfaction'\n",
    "if text_col not in df.columns or label_col not in df.columns:\n",
    "    raise Exception(f'Columns not found: Reviews or Satisfaction. Available columns: {df.columns.tolist()}')\n",
    "print(f'Using text column: {text_col}, label column: {label_col}')\n",
    "\n",
    "# --- 4. Clean and convert labels to binary if needed ---\n",
    "df = df.dropna(subset=[text_col, label_col])\n",
    "if df[label_col].dtype in ['int64', 'float64']:\n",
    "    median_rating = df[label_col].median()\n",
    "    df['binary_sentiment'] = (df[label_col] > median_rating).astype(int)\n",
    "    label_col = 'binary_sentiment'\n",
    "else:\n",
    "    from sklearn.preprocessing import LabelEncoder\n",
    "    le = LabelEncoder()\n",
    "    df['encoded_labels'] = le.fit_transform(df[label_col])\n",
    "    label_col = 'encoded_labels'\n",
    "\n",
    "# --- 5. Train/test split and tokenize ---\n",
    "X = df[text_col].astype(str).values\n",
    "y = df[label_col].values\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "X_train = [text_to_word_sequence(text) for text in X_train]\n",
    "X_test = [text_to_word_sequence(text) for text in X_test]\n",
    "\n",
    "print(f'Train samples: {len(X_train)}, Test samples: {len(X_test)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. [Model 1] Word2Vec Training\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instantiation and Functions for Embedding and Padding\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Functions to convert your training and test data into something you can feed into a RNN.\n",
    "The functions**  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "word2vec = Word2Vec(sentences=X_train, vector_size=60, min_count=10, window=10)\n",
    "\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import numpy as np\n",
    "\n",
    "# Function to convert a sentence (list of words) into a matrix representing the words in the embedding space\n",
    "def embed_sentence(word2vec, sentence):\n",
    "    embedded_sentence = []\n",
    "    for word in sentence:\n",
    "        if word in word2vec.wv:\n",
    "            embedded_sentence.append(word2vec.wv[word])\n",
    "\n",
    "    return np.array(embedded_sentence)\n",
    "\n",
    "# Function that converts a list of sentences into a list of matrices\n",
    "def embedding(word2vec, sentences):\n",
    "    embed = []\n",
    "\n",
    "    for sentence in sentences:\n",
    "        embedded_sentence = embed_sentence(word2vec, sentence)\n",
    "        embed.append(embedded_sentence)\n",
    "\n",
    "    return embed\n",
    "\n",
    "# Embed the training and test sentences\n",
    "X_train_embed = embedding(word2vec, X_train)\n",
    "X_test_embed = embedding(word2vec, X_test)\n",
    "\n",
    "\n",
    "# Pad the training and test embedded sentences\n",
    "X_train_pad = pad_sequences(X_train_embed, dtype='float32', padding='post', maxlen=200)\n",
    "X_test_pad = pad_sequences(X_test_embed, dtype='float32', padding='post', maxlen=200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚òùÔ∏è To be sure that it worked, let's check the following for `X_train_pad` and `X_test_pad`:\n",
    "- they are numpy arrays\n",
    "- they are 3-dimensional\n",
    "- the last dimension is of the size of your word2vec embedding space (you can get it with `word2vec.wv.vector_size`\n",
    "- the first dimension is of the size of your `X_train` and `X_test`\n",
    "\n",
    "‚úÖ **Good Practice** ‚úÖ Such tests are quite important! Not only in this exercise, but in real-life applications. It prevents from finding errors too late and from letting them propagate through the entire notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEST ME\n",
    "for X in [X_train_pad, X_test_pad]:\n",
    "    assert type(X) == np.ndarray\n",
    "    assert X.shape[-1] == word2vec.wv.vector_size\n",
    "\n",
    "\n",
    "assert X_train_pad.shape[0] == len(X_train)\n",
    "assert X_test_pad.shape[0] == len(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. [Model 2] Trained Word2Vec ###Transfer Learning\n",
    "\n",
    "Your accuracy, while above the model 1, might be quite low. There are multiple options to improve it, as data cleaning and improving the quality of the embedding.\n",
    "\n",
    "We won't dig into data cleaning strategies here. Let's try to improve the quality of our embedding. But instead of just loading a larger corpus, why not benefiting from the embedding that others have learned? \n",
    "\n",
    "Because, the quality of an embedding, i.e. the proximity of the words, can be derived from different tasks. This is exactly what transfer learning is.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found file: /home/xf/code/TomX79/06-Deep-Learning/04-RNN-and-NLP/data-sentiment-analysis-with-word2vec/Data_Set_webmd_6174_Rows_Randomised.csv\n",
      "Loaded as CSV\n",
      "Columns: ['Random Number', 'Age', 'Condition', 'Date', 'Drug', 'DrugId', 'EaseofUse', 'Effectiveness', 'Reviews', 'Satisfaction', 'Sex', 'Sides']\n",
      "   Random Number    Age   Condition      Date  \\\n",
      "0       0.000004  55-64  Depression  05-01-10   \n",
      "1       0.000008  25-34  Depression  12-02-08   \n",
      "2       0.000049  35-44  Depression  10-09-09   \n",
      "3       0.000104  45-54  Depression  03-08-12   \n",
      "4       0.000140  35-44  Depression  01-11-08   \n",
      "\n",
      "                                        Drug  DrugId  EaseofUse  \\\n",
      "0  bupropion hcl sr tablet, extended release   13507          5   \n",
      "1                              wellbutrin xl   76851          5   \n",
      "2                                     celexa    8603          5   \n",
      "3                                     prozac    6997          1   \n",
      "4                              fluoxetine dr    1774          2   \n",
      "\n",
      "   Effectiveness                                            Reviews  \\\n",
      "0              2  I was switched from Lexapro to Bupropion about...   \n",
      "1              5  Was taking Prozac and it was ok but low sex dr...   \n",
      "2              5  I am not sure but I think it has caused a fine...   \n",
      "3              1  i am trying to find out about this medication ...   \n",
      "4              1                                                      \n",
      "\n",
      "   Satisfaction     Sex                                              Sides  \n",
      "0             2  Female  Dry mouth ,  sore throat ,  dizziness ,  nause...  \n",
      "1             5  Female  Dry mouth ,  sore throat ,  dizziness ,  nause...  \n",
      "2             5  Female  Nausea ,  dry mouth , loss of appetite, tiredn...  \n",
      "3             1  Female  Nausea , drowsiness,  dizziness ,  anxiety ,  ...  \n",
      "4             1  Female  Nausea , drowsiness,  dizziness ,  anxiety ,  ...  \n",
      "Using text column: Reviews, label column: Satisfaction\n",
      "Train samples: 4938, Test samples: 1235\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.preprocessing.text import text_to_word_sequence\n",
    "\n",
    "# --- 1. Locate and load the file ---\n",
    "possible_paths = [\n",
    "    '/home/xf/code/TomX79/06-Deep-Learning/04-RNN-and-NLP/data-sentiment-analysis-with-word2vec/Data_Set_webmd_6174_Rows_Randomised.csv'\n",
    " ]\n",
    "file_path = None\n",
    "for path in possible_paths:\n",
    "    if os.path.exists(path):\n",
    "        file_path = path\n",
    "        print(f'Found file: {file_path}')\n",
    "        break\n",
    "if file_path is None:\n",
    "    raise FileNotFoundError('Data_Set_webmd_6174_Rows_Randomised.csv file not found!')\n",
    "\n",
    "# --- 2. Try loading as CSV, then TSV ---\n",
    "try:\n",
    "    df = pd.read_csv(file_path)\n",
    "    print('Loaded as CSV')\n",
    "except Exception:\n",
    "    try:\n",
    "        df = pd.read_csv(file_path, sep='\\t')\n",
    "        print('Loaded as TSV')\n",
    "    except Exception as e:\n",
    "        print('Could not load file:', e)\n",
    "        raise\n",
    "\n",
    "print('Columns:', df.columns.tolist())\n",
    "print(df.head())\n",
    "\n",
    "# --- 3. Use explicit columns for MODEL 2 ---\n",
    "text_col = 'Reviews'\n",
    "label_col = 'Satisfaction'\n",
    "if text_col not in df.columns or label_col not in df.columns:\n",
    "    raise Exception(f'Columns not found: Reviews or Satisfaction. Available columns: {df.columns.tolist()}')\n",
    "print(f'Using text column: {text_col}, label column: {label_col}')\n",
    "\n",
    "# --- 4. Clean and convert labels to binary if needed ---\n",
    "df = df.dropna(subset=[text_col, label_col])\n",
    "if df[label_col].dtype in ['int64', 'float64']:\n",
    "    median_rating = df[label_col].median()\n",
    "    df['binary_sentiment'] = (df[label_col] > median_rating).astype(int)\n",
    "    label_col = 'binary_sentiment'\n",
    "else:\n",
    "    from sklearn.preprocessing import LabelEncoder\n",
    "    le = LabelEncoder()\n",
    "    df['encoded_labels'] = le.fit_transform(df[label_col])\n",
    "    label_col = 'encoded_labels'\n",
    "\n",
    "# --- 5. Train/test split and tokenize ---\n",
    "X = df[text_col].astype(str).values\n",
    "y = df[label_col].values\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "X_train = [text_to_word_sequence(text) for text in X_train]\n",
    "X_test = [text_to_word_sequence(text) for text in X_test]\n",
    "\n",
    "print(f'Train samples: {len(X_train)}, Test samples: {len(X_test)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['fasttext-wiki-news-subwords-300', 'conceptnet-numberbatch-17-06-300', 'word2vec-ruscorpora-300', 'word2vec-google-news-300', 'glove-wiki-gigaword-50', 'glove-wiki-gigaword-100', 'glove-wiki-gigaword-200', 'glove-wiki-gigaword-300', 'glove-twitter-25', 'glove-twitter-50', 'glove-twitter-100', 'glove-twitter-200', '__testing_word2vec-matrix-synopsis']\n"
     ]
    }
   ],
   "source": [
    "import gensim.downloader as api\n",
    "print(list(api.info()['models'].keys()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ÑπÔ∏è You can also find the list of the models and their size on the [`gensim-data` repository](https://github.com/RaRe-Technologies/gensim-data#models)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùì **Question** ‚ùì Load one of the pre-trained word2vec embedding spaces. \n",
    "\n",
    "You can do that with `api.load(the-model-of-your-choice)`, and store it in `word2vec_transfer`\n",
    "\n",
    "<details>\n",
    "    <summary>üí° Hint</summary>\n",
    "    \n",
    "The `glove-wiki-gigaword-50` model is a good candidate to start with as it is smaller (65 MB).\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [],
   "source": [
    "word2vec_transfer = api.load(\"glove-wiki-gigaword-50\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùì **Question** ‚ùì Check the size of the vocabulary, but also the size of the embedding space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "400000\n",
      "50\n"
     ]
    }
   ],
   "source": [
    "print(len(word2vec_transfer.key_to_index))\n",
    "print(len(word2vec_transfer['art']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Text Processing for RNN\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùì Let's embed `X_train` and `X_test`, same as in the first question where we provided the functions to do so! (There is a slight difference in the `embed_sentence_with_TF` function that we will not dig into)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to convert a sentence (list of words) into a matrix representing the words in the embedding space\n",
    "def embed_sentence_with_TF(word2vec, sentence):\n",
    "    embedded_sentence = []\n",
    "    for word in sentence:\n",
    "        if word in word2vec:\n",
    "            embedded_sentence.append(word2vec[word])\n",
    "\n",
    "    return np.array(embedded_sentence)\n",
    "\n",
    "# Function that converts a list of sentences into a list of matrices\n",
    "def embedding(word2vec, sentences):\n",
    "    embed = []\n",
    "\n",
    "    for sentence in sentences:\n",
    "        embedded_sentence = embed_sentence_with_TF(word2vec, sentence)\n",
    "        embed.append(embedded_sentence)\n",
    "\n",
    "    return embed\n",
    "\n",
    "# Embed the training and test sentences\n",
    "X_train_embed_2 = embedding(word2vec_transfer, X_train)\n",
    "X_test_embed_2 = embedding(word2vec_transfer, X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Focus**   Do not forget to pad your results and store it in `X_train_pad_2` and `X_test_pad_2`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [],
   "source": [
    "# Pad the training and test embedded sentences\n",
    "X_train_pad_2 = pad_sequences(X_train_embed_2, dtype='float32', padding='post', maxlen=200)\n",
    "X_test_pad_2 = pad_sequences(X_test_embed_2, dtype='float32', padding='post', maxlen=200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Neural Network Training\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùì **Question** ‚ùì Reinitialize a model and fit it on your new embedded (and padded) data!  Evaluate it on your test set and compare it to your previous accuracy.\n",
    "\n",
    "‚ùó **Remark** ‚ùó The training here could take some time. You can just compute 10 epochs (this is **not** a good practice, it is just not to wait too long) and go to the next exercise while it trains - or take a break, you probably deserve it ;)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Masking\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import numpy as np\n",
    "\n",
    "def init_model():\n",
    "    # Infer input shape from X_train_pad_2\n",
    "    input_shape = (X_train_pad_2.shape[1], X_train_pad_2.shape[2])\n",
    "    # Infer output shape (binary or multiclass)\n",
    "    n_classes = len(np.unique(y_train))\n",
    "    model = Sequential()\n",
    "    model.add(Masking(mask_value=0., input_shape=input_shape))\n",
    "    model.add(LSTM(64, return_sequences=False))\n",
    "    if n_classes == 2:\n",
    "        model.add(Dense(1, activation='sigmoid'))\n",
    "        model.compile(optimizer=Adam(learning_rate=0.001), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    else:\n",
    "        model.add(Dense(n_classes, activation='softmax'))\n",
    "        model.compile(optimizer=Adam(learning_rate=0.001), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "\u001b[1m108/108\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 128ms/step - accuracy: 0.5319 - loss: 0.6916 - val_accuracy: 0.6019 - val_loss: 0.6695\n",
      "Epoch 2/20\n",
      "\u001b[1m108/108\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 128ms/step - accuracy: 0.5319 - loss: 0.6916 - val_accuracy: 0.6019 - val_loss: 0.6695\n",
      "Epoch 2/20\n",
      "\u001b[1m108/108\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 116ms/step - accuracy: 0.6185 - loss: 0.6605 - val_accuracy: 0.6262 - val_loss: 0.6552\n",
      "Epoch 3/20\n",
      "\u001b[1m108/108\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 116ms/step - accuracy: 0.6185 - loss: 0.6605 - val_accuracy: 0.6262 - val_loss: 0.6552\n",
      "Epoch 3/20\n",
      "\u001b[1m108/108\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 118ms/step - accuracy: 0.6363 - loss: 0.6352 - val_accuracy: 0.6370 - val_loss: 0.6487\n",
      "Epoch 4/20\n",
      "\u001b[1m108/108\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 118ms/step - accuracy: 0.6363 - loss: 0.6352 - val_accuracy: 0.6370 - val_loss: 0.6487\n",
      "Epoch 4/20\n",
      "\u001b[1m108/108\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 117ms/step - accuracy: 0.6505 - loss: 0.6315 - val_accuracy: 0.6491 - val_loss: 0.6352\n",
      "Epoch 5/20\n",
      "\u001b[1m108/108\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 117ms/step - accuracy: 0.6505 - loss: 0.6315 - val_accuracy: 0.6491 - val_loss: 0.6352\n",
      "Epoch 5/20\n",
      "\u001b[1m108/108\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 116ms/step - accuracy: 0.6727 - loss: 0.6078 - val_accuracy: 0.6748 - val_loss: 0.6203\n",
      "Epoch 6/20\n",
      "\u001b[1m108/108\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 116ms/step - accuracy: 0.6727 - loss: 0.6078 - val_accuracy: 0.6748 - val_loss: 0.6203\n",
      "Epoch 6/20\n",
      "\u001b[1m108/108\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 116ms/step - accuracy: 0.6802 - loss: 0.6021 - val_accuracy: 0.6633 - val_loss: 0.6216\n",
      "Epoch 7/20\n",
      "\u001b[1m108/108\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 116ms/step - accuracy: 0.6802 - loss: 0.6021 - val_accuracy: 0.6633 - val_loss: 0.6216\n",
      "Epoch 7/20\n",
      "\u001b[1m108/108\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 117ms/step - accuracy: 0.6965 - loss: 0.5920 - val_accuracy: 0.6538 - val_loss: 0.6311\n",
      "Epoch 8/20\n",
      "\u001b[1m108/108\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 117ms/step - accuracy: 0.6965 - loss: 0.5920 - val_accuracy: 0.6538 - val_loss: 0.6311\n",
      "Epoch 8/20\n",
      "\u001b[1m108/108\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 118ms/step - accuracy: 0.7004 - loss: 0.5803 - val_accuracy: 0.6842 - val_loss: 0.6109\n",
      "Epoch 9/20\n",
      "\u001b[1m108/108\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 118ms/step - accuracy: 0.7004 - loss: 0.5803 - val_accuracy: 0.6842 - val_loss: 0.6109\n",
      "Epoch 9/20\n",
      "\u001b[1m108/108\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 129ms/step - accuracy: 0.7109 - loss: 0.5570 - val_accuracy: 0.6856 - val_loss: 0.6053\n",
      "Epoch 10/20\n",
      "\u001b[1m108/108\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 129ms/step - accuracy: 0.7109 - loss: 0.5570 - val_accuracy: 0.6856 - val_loss: 0.6053\n",
      "Epoch 10/20\n",
      "\u001b[1m108/108\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 191ms/step - accuracy: 0.7326 - loss: 0.5402 - val_accuracy: 0.6646 - val_loss: 0.6534\n",
      "Epoch 11/20\n",
      "\u001b[1m108/108\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 191ms/step - accuracy: 0.7326 - loss: 0.5402 - val_accuracy: 0.6646 - val_loss: 0.6534\n",
      "Epoch 11/20\n",
      "\u001b[1m108/108\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 127ms/step - accuracy: 0.7338 - loss: 0.5343 - val_accuracy: 0.6835 - val_loss: 0.6421\n",
      "Epoch 12/20\n",
      "\u001b[1m108/108\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 127ms/step - accuracy: 0.7338 - loss: 0.5343 - val_accuracy: 0.6835 - val_loss: 0.6421\n",
      "Epoch 12/20\n",
      "\u001b[1m108/108\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 163ms/step - accuracy: 0.7670 - loss: 0.4812 - val_accuracy: 0.6916 - val_loss: 0.6093\n",
      "Epoch 13/20\n",
      "\u001b[1m108/108\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 163ms/step - accuracy: 0.7670 - loss: 0.4812 - val_accuracy: 0.6916 - val_loss: 0.6093\n",
      "Epoch 13/20\n",
      "\u001b[1m108/108\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 172ms/step - accuracy: 0.7702 - loss: 0.4793 - val_accuracy: 0.6592 - val_loss: 0.6511\n",
      "Epoch 14/20\n",
      "\u001b[1m108/108\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 172ms/step - accuracy: 0.7702 - loss: 0.4793 - val_accuracy: 0.6592 - val_loss: 0.6511\n",
      "Epoch 14/20\n",
      "\u001b[1m108/108\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 168ms/step - accuracy: 0.7935 - loss: 0.4535 - val_accuracy: 0.6923 - val_loss: 0.6320\n",
      "\u001b[1m108/108\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 168ms/step - accuracy: 0.7935 - loss: 0.4535 - val_accuracy: 0.6923 - val_loss: 0.6320\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x7280ca0e83b0>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "es = EarlyStopping(patience=5, restore_best_weights=True)\n",
    "\n",
    "model = init_model()\n",
    "\n",
    "model.fit(X_train_pad_2, y_train,\n",
    "          batch_size = 32,\n",
    "          epochs=20,\n",
    "          validation_split=0.3,\n",
    "          callbacks=[es]\n",
    "         )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Evaluation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "üèÜ MODEL PERFORMANCE RESULTS üèÜ\n",
      "============================================================\n",
      "üìà Test Set Accuracy: 68.664%\n",
      "------------------------------------------------------------\n",
      "üìä Performance Rating: ‚ö†Ô∏è FAIR\n",
      "üéØ Achieved: 68.664% accuracy on unseen data\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# üéØ Model Performance Evaluation\n",
    "res = model.evaluate(X_test_pad_2, y_test, verbose=0)\n",
    "\n",
    "# üìä Display Results with Beautiful Formatting\n",
    "accuracy = res[1] * 100\n",
    "print(\"=\"*60)\n",
    "print(\"üèÜ MODEL PERFORMANCE RESULTS üèÜ\")\n",
    "print(\"=\"*60)\n",
    "print(f\"üìà Test Set Accuracy: {accuracy:.3f}%\")\n",
    "print(\"-\"*60)\n",
    "\n",
    "# üé® Performance Rating\n",
    "if accuracy >= 80:\n",
    "    rating = \"üåü EXCELLENT\"\n",
    "    emoji = \"üöÄ\"\n",
    "elif accuracy >= 70:\n",
    "    rating = \"‚úÖ GOOD\"\n",
    "    emoji = \"üëç\"\n",
    "elif accuracy >= 60:\n",
    "    rating = \"‚ö†Ô∏è FAIR\"\n",
    "    emoji = \"üìä\"\n",
    "else:\n",
    "    rating = \"‚ùå NEEDS IMPROVEMENT\"\n",
    "    emoji = \"üìâ\"\n",
    "\n",
    "print(f\"{emoji} Performance Rating: {rating}\")\n",
    "print(f\"üéØ Achieved: {accuracy:.3f}% accuracy on unseen data\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because your new word2vec has been trained on a large corpus, it has a representation for many many words! Way more than with your small dataset, especially as you discarded words that were not present more than a given number of times in the train set. For that reason, you have way more embedded words in your train and test set, which makes each iteration longer than previously\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "drug-analysis-review",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
